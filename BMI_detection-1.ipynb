{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c437355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b2d5cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr= 0.000025341814210920752\n",
    "epoch = 68\n",
    "resize = 148\n",
    "dropout= 0.02071552291233333\n",
    "\n",
    "\n",
    "maxpooling= [2,5]\n",
    "\n",
    "batch_size = 128\n",
    "num_filters= [18,5]\n",
    "kernel_size = [10,8]\n",
    "resize = 148\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ee68986",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((resize, resize)),  # Chọn kích thước ảnh sau khi augment\n",
    "    transforms.ToTensor(),  # Chuyển ảnh sang tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Chuẩn hóa tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef2e6005",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMIModel(nn.Module):\n",
    "    def __init__(self, resize = 148, num_filters = [18,5], kernel_size =[10,8], maxpooling = [2,5]):\n",
    "        super().__init__()\n",
    "        self.resize = resize\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.maxpooling = maxpooling\n",
    "        \n",
    "        \n",
    "        # input -> conv2d -> batchnorm -> maxpool2d ->conv2d -> batchnorm -> maxpool \n",
    "        \n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "                            in_channels= 3, \n",
    "                            out_channels=self.num_filters[0], \n",
    "                            kernel_size=self.kernel_size[0],\n",
    "                            )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "                            in_channels=self.num_filters[0], \n",
    "                            out_channels=self.num_filters[1], \n",
    "                            kernel_size=self.kernel_size[1],\n",
    "                            )\n",
    "        # self.conv3 = nn.Conv2d(\n",
    "        #                     in_channels=self.num_filters[1], \n",
    "        #                     out_channels=1, \n",
    "        #                     kernel_size=3,\n",
    "        #                     )\n",
    "        # nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        # nn.init.zeros_(self.conv1.bias)\n",
    "        # nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        # nn.init.zeros_(self.conv2.bias)\n",
    "\n",
    "        self.f_activation1 = nn.ReLU()\n",
    "        self.f_activation2 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        # torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)\n",
    "        self.batchNorm1 = nn.BatchNorm2d(num_features=self.num_filters[0])\n",
    "        self.batchNorm2 = nn.BatchNorm2d(num_features=self.num_filters[1])\n",
    "\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "        \n",
    "        self.maxpooling1 = nn.MaxPool2d(kernel_size=self.maxpooling[0])\n",
    "        self.maxpooling2 = nn.MaxPool2d(kernel_size=self.maxpooling[1])\n",
    "        \n",
    "        self.h_conv1 = int((self.resize- self.kernel_size[0]) + 1)\n",
    "        #self.w_conv1 = int((self.shape_data[2] - self.kernel_size[0]) + 1)\n",
    "\n",
    "        self.h_maxpool1 = int((self.h_conv1 - self.maxpooling[0])/self.maxpooling[0] + 1)\n",
    "        #self.w_maxpool1 = int((self.w_conv1 - self.maxpooling[0])/self.maxpooling[0] + 1)  \n",
    "\n",
    "        self.h_conv2 = int((self.h_maxpool1 - self.kernel_size[1]) + 1)\n",
    "        #self.w_conv2 = int((self.w_maxpool1 - self.kernel_size[1]) + 1)\n",
    "\n",
    "        self.h_maxpool2= int((self.h_conv2 - self.maxpooling[1])/self.maxpooling[1] + 1)\n",
    "        #self.w_maxpool2 = int((self.w_conv2 - self.maxpooling[1])/self.maxpooling[1] + 1)  \n",
    "\n",
    "        # self.h_conv3 = int((self.h_maxpool2 - 3) + 1)\n",
    "        # self.w_conv3 = int((self.w_maxpool2 - 3) + 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.linear1 = nn.Linear(self.h_maxpool2*self.h_maxpool2*num_filters[1], 120)\n",
    "        self.f_activation3 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(120, 84)\n",
    "        self.f_activation4= nn.ReLU()\n",
    "        self.linear3 = nn.Linear(84, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.f_activation1(x)\n",
    "        x = self.batchNorm1(x)\n",
    "        x = self.maxpooling1(x)\n",
    "        \n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.f_activation2(x)\n",
    "        x = self.batchNorm2(x)\n",
    "        x = self.maxpooling2(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e079d1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BMIModel()\n",
    "model.load_state_dict(torch.load('(10,8)kernel_(18,5)filter_(2,5)maxpool_2.5341814210920752e-05lr.pt',  map_location=torch.device('cpu'))) # it takes the loaded dictionary, not the path file itself\n",
    "model.eval()\n",
    "face_detection = YOLO('best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33369bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[23.8889]])\n"
     ]
    }
   ],
   "source": [
    "# Nhan dien BMI qua hinh anh khuon mat\n",
    "with torch.no_grad():\n",
    "    image = Image.open('m_021.jpg').convert(\"RGB\")\n",
    "   \n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze_(0)\n",
    "    y = model(image)\n",
    "\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2efa1fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\DELL\\AI\\Project\\BMI\\img_2541.bmp: 640x576 1 FACE, 371.7ms\n",
      "Speed: 10.8ms preprocess, 371.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n"
     ]
    }
   ],
   "source": [
    "# Cat hinh anh khuon mat qua hinh anh toan than\n",
    "img_path = \"img_2541.bmp\"\n",
    "\n",
    "results = face_detection(img_path) \n",
    "boxes = results[0].boxes\n",
    "img = cv2.imread(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d448456b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[22.7053]])\n"
     ]
    }
   ],
   "source": [
    "# Nhan dien BMI qua hinh anh da cat\n",
    "for box in boxes:\n",
    "    top_left_x = int(box.xyxy.tolist()[0][0])\n",
    "    top_left_y = int(box.xyxy.tolist()[0][1])\n",
    "    bottom_right_x = int(box.xyxy.tolist()[0][2])\n",
    "    bottom_right_y = int(box.xyxy.tolist()[0][3])\n",
    "    croppedImg = img[top_left_y+1:bottom_right_y, top_left_x+1:bottom_right_x]\n",
    "    cv2.rectangle(img, (top_left_x, top_left_y), (bottom_right_x, bottom_right_y), (50, 200, 129), 2)\n",
    "cv2.imwrite(f\"Cropped.jpg\", croppedImg)\n",
    "with torch.no_grad():\n",
    "    image = Image.open(\"Cropped.jpg\").convert(\"RGB\")\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze_(0)\n",
    "    bmi_output = model(image)\n",
    "    bmi_value = bmi_output.item()\n",
    "    print(bmi_output)\n",
    "    \n",
    "for box in boxes:\n",
    "    top_left_x = int(box.xyxy.tolist()[0][0])\n",
    "    top_left_y = int(box.xyxy.tolist()[0][1])\n",
    "    bottom_right_x = int(box.xyxy.tolist()[0][2])\n",
    "    bottom_right_y = int(box.xyxy.tolist()[0][3])\n",
    "    croppedImg = img[top_left_y+1:bottom_right_y, top_left_x+1:bottom_right_x]\n",
    "    cv2.rectangle(img, (top_left_x, top_left_y), (bottom_right_x, bottom_right_y), (50, 200, 129), 1)\n",
    "    # Display BMI result on the original image\n",
    "bmi_text = f'BMI: {bmi_value:.2f}'\n",
    "cv2.putText(img, bmi_text, (top_left_x, top_left_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 255, 0), 1)\n",
    "cv2.imshow('window', img)\n",
    "cv2.imwrite(f\"BMI.jpg\", img)\n",
    "cv2.waitKey(0) \n",
    "  \n",
    "# closing all open windows \n",
    "cv2.destroyAllWindows() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35069e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3108e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
